{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We can download the 20news data by the tools like fetch_20newsgroup. Note, **we are going to remove the headers and footers in order they contain straightforward words which indicate the labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers'),\n",
    "                                  shuffle=True, random_state=11)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers',  'footers'),\n",
    "                                  shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text number: 11314\n",
      "Testing text number: 7532\n"
     ]
    }
   ],
   "source": [
    "print('Training text number:', len(newsgroups_train.data))\n",
    "print('Testing text number:', len(newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk.religion.misc'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names[newsgroups_train.target[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In article <1r3le9$mlj@horus.ap.mchp.sni.de> frank@D012S658.uucp (Frank O'Dwyer) writes:\n",
      ">In article <1r22qp$4sk@squick.eitech.com> ekr@squick.eitech.com (Eric Rescorla) writes:\n",
      ">#In article <1r0m89$r0o@horus.ap.mchp.sni.de> frank@D012S658.uucp (Frank O'Dwyer) writes:\n",
      ">#>In article <1qvu33$jk3@kyle.eitech.com> ekr@kyle.eitech.com (Eric Rescorla) writes:\n",
      ">#>#>If almost all people agree that the sun exists (in the usual, uncritical sense),\n",
      ">#>#>and almost all people agree that  a deal is bad,  it's a reasonable \n",
      ">#>#>conclusion that the sun really does exist, and that the deal really is bad.\n",
      ">#>#I disagree completely. Until rather recently, most people did not\n",
      ">#>#believe in evolution or the possibility of the atom bomb. Popular\n",
      ">#>#opinion is notoriously wrong about matters of fact.\n",
      ">#>True, but nevertheless the basis of all \"matters of fact\" is overwhelming\n",
      ">#>popular opinion, and some overwhelming popular opinion *is* fact (\"the\n",
      ">#>sun shines\").  If it were not so, physics would be a personal matter,\n",
      ">#>assumed to be different for each of us.  There would be YourGravity and\n",
      ">#>MyGravity and no theoretical framework to encompass them and predict\n",
      ">#>both.  \n",
      ">#This is simply complete nonsense. The basis for 'matters of fact' is,\n",
      ">#if any class of opinion, the majority of INFORMED popular opinion\n",
      ">#for some value of informed. I would really hate to base my knowledge\n",
      ">#of, for instance, QM on what the overwhelming popular opinion is.\n",
      ">The *basis*, Eric, is people peering at the world and saying what\n",
      ">they see.  I'm talking about uninterpreted facts - observations.  _People_\n",
      ">do those.  Agreement on some observations is a prerequisite for a theory\n",
      ">that is more than personal.\n",
      "Yes, that's true, but you have to be clear exactly what is\n",
      "an uninterpreted observation. It's pretty low level stuff.\n",
      "'The sun shines' is already a LOT higher level than that. We\n",
      "can agree that 'I perceive brightness' perhaps.\n",
      "\n",
      ">#>Now I take an experience of good/evil to be every bit as raw a fact as an\n",
      ">#>experience of pain, or vision.\n",
      ">#That might seem like a good first pass guess, but it turns out to\n",
      ">#be a pretty cruddy way to look at things, because we all seem to\n",
      ">#have rather different opinions (experiences) about what is good\n",
      ">#and evil, while we seem to be able to agree on what the meter says.\n",
      ">You're not comparing apples with apples.  If we all look at the same meter,\n",
      ">we'll agree.  If we're all in the same situation, that's when we'll\n",
      ">agree on fundamental values, if at all.  People who say that nobody agrees on\n",
      ">values to the same extent that they agree on trivial observations seem\n",
      ">to be unaware of the extent of agreement on either.\n",
      "Huh? What do you mean 'all in the same situation?' Let's take me\n",
      "and Dennis Kriz as examples. We're in pretty different situations,\n",
      "but I think we can agree as to whether it's day or night. I don't\n",
      "think we can agree as to whether or not abortion is morally\n",
      "acceptable. Yet we are certainly in the same difference of\n",
      "situations with respect to each other. Looks like weasel-words\n",
      "to me, Frank. \n",
      "\n",
      ">#I don't see that it's any evidence at all.\n",
      ">#As I point out above, I'm really not interested very much in\n",
      ">#what the popular opinion is. I'm prepared to trust--to some extent--\n",
      ">#the popular opinion about direct matters of physical observation\n",
      ">#because by and large they accord with my own. However, if everyone\n",
      ">#else said the dial read 1.5 and it looked like a 3 to me, I would\n",
      ">#hope that I would believe myself. I.e. believing other people about\n",
      ">#these matters seems to have a reasonable probability of predicting\n",
      ">#what I would believe if I observed myself, but the possibility exists\n",
      ">#that it is not. Since I know from observation that others disagree\n",
      ">#with me about what is good, I believe I can discount popular opinion\n",
      ">#about 'good' from the beginning as a predictor of my opinion.\n",
      ">#I would say that the fact that it seems almost impossible to get\n",
      ">#people to agree on what is good in a really large number of situations\n",
      ">#is probably the best evidence that objective morality is bogus, actually.\n",
      ">Firstly, if everyone else said the dial was 1.5 and I saw 3, I'd check\n",
      ">my lens prescription.\n",
      "That's up to you, I guess.\n",
      "\n",
      ">  Secondly, your observation that people\n",
      ">disagree shows nothing - people may be looking at different things,\n",
      ">by virtue of being in different situations.  If I look at an elephant, I'll \n",
      ">see an elephant.  That doesn't imply that you will see an elephant if you \n",
      ">look at an iguana.\n",
      "This 'different situations' stuff is pretty confusing, Frank. How\n",
      "do we decide if we are in the same situation? You mind explaining?\n",
      "\n",
      ">    Thirdly, I question your assumption that when\n",
      ">people disagree about how to achieve fundamental or secondary goals, that \n",
      ">they therefore do not have the same fundamental goals (that seems to be the \n",
      ">disagreement you refer to).\n",
      "Huh? I don't think so. I think that people disagree about\n",
      "fundamental goals.\n",
      "\n",
      "-Ekr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text_lenghts =list(map(len, newsgroups_train.data))\n",
    "test_text_lenghts =list(map(len, newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  109.  ,   294.  ,   436.  ,   820.  ,  1512.75,  4393.75,  8301.92])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(train_text_lenghts, [3, 15, 25, 50, 75, 95, 98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c3381cc898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(train_text_lenghts, bins=15)\n",
    "_ = plt.title('Text Lengths Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of texts have lengths less than 10000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split News into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import nltk.data\n",
    "class splitTexts:\n",
    "    '''Split articles into sentences'''\n",
    "    def __init__(self):\n",
    "        self.tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        \n",
    "    def __preprocess(self, text):\n",
    "        pattern1 = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,5}'\n",
    "        regex1 = re.compile(pattern1, flags=re.IGNORECASE)\n",
    "        text = regex1.sub('email', text)\n",
    "        pattern2 = '[|#$@><=+-]'\n",
    "        regex2 = re.compile(pattern2)\n",
    "        text = regex2.sub(' ', text)\n",
    "        pattern3 = r'\\n'\n",
    "        regex3 = re.compile(pattern3)\n",
    "        text = regex3.sub(' ', text)\n",
    "        pattern4 = r'[0-9]'\n",
    "        regex4 = re.compile(pattern4)\n",
    "        text = regex4.sub(' ', text)\n",
    "        pattern5 = r'[ ]+'\n",
    "        regex5 = re.compile(pattern5)\n",
    "        text = regex5.sub(' ', text)\n",
    "        return text.lower()\n",
    "    \n",
    "    def split(self, text):\n",
    "        text = self.__preprocess(text)\n",
    "        sentences = self.tokenizer.tokenize(text)\n",
    "        return sentences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitSent = splitTexts()\n",
    "train_news_sents = [splitSent.split(news) for news in newsgroups_train.data]\n",
    "test_news_sents = [splitSent.split(news) for news in newsgroups_test.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1c343b08940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD44B8e5yw2qHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c339351cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Calculate the lengths of each news\n",
    "news_sent_num = list(map(len, train_news_sents))\n",
    "_ = plt.hist(news_sent_num, bins=10)\n",
    "plt.title('Histogram of News Sentence Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.,   8.,  15.,  43.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(news_sent_num, [25, 50, 75, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most news has less than 100 sentences. Actually, 95% news has less than 40 sentences. We need to remove those news with a lot of sentences because two much length may counter the memory of RNNmodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(news_sent_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split sentences into words\n",
    "\n",
    "First, we need to filter out those news without much information, namely news of one or two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(train_news_sents, newsgroups_train.target))\n",
    "test_data = list(zip(test_news_sents, newsgroups_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove news with  more than 50 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Filter those empty texts and texts of too many sentences\n",
    "train_data = list(filter(lambda x: len(x[0])>0 and len(x[0])<50, train_data))\n",
    "test_data = list(filter(lambda x: len(x[0])>0 and len(x[0])<50, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample Number 10834\n",
      "Testing Sample Number 7238\n"
     ]
    }
   ],
   "source": [
    "print('Training Sample Number', len(train_data))\n",
    "print('Testing Sample Number', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dozens of news has been filtered out due to the small size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class splitSent:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = WordPunctTokenizer().tokenize\n",
    "        self.lemmatizer = WordNetLemmatizer().lemmatize\n",
    "        \n",
    "    def __preprocess(self, sent):\n",
    "        return None\n",
    "    \n",
    "    def split(self, sent):\n",
    "        '''\n",
    "        Split a sentence into words\n",
    "        Lemmatize words\n",
    "        '''\n",
    "        words = self.tokenizer(sent)    \n",
    "        words = list(map(self.lemmatizer, words))\n",
    "        return words\n",
    "    \n",
    "    def splitList(self, sents):\n",
    "        sents_words = list(map(self.split, sents))\n",
    "        return sents_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'dog', 'and', 'cat', 'very', 'much', '!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitsent = splitSent()\n",
    "splitsent.split('I love dogs and cats very much!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize each sentence into words and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hier_train_data = [(splitsent.splitList(item[0]), item[1]) for item in train_data]\n",
    "hier_test_data = [(splitsent.splitList(item[0]), item[1]) for item in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform words to IDs\n",
    "\n",
    "Words and sentences are tokens which can be understood by humans but not computers. Therefore, we need to transform them into numbers that can be handled by computers. First of all, we need to build a vocabulary and map each word to an ID, then we can represent each sentence with a sequence of IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter, Iterable\n",
    "class text2ids:\n",
    "    '''\n",
    "    Provide tools to split sents and build vocabulary\n",
    "    To map a sent into sequence if Ids\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def __buildVocabulary(self, text, vocab_size, min_freq):\n",
    "        '''Unlist all the nested list and build a vocabulary'''\n",
    "        tokens = []\n",
    "        for sent in text:\n",
    "            for words in sent:\n",
    "                tokens.extend(words)\n",
    "        #tokens = list(itertools.chain.from_iterable(text))\n",
    "        token_freq = Counter(tokens)\n",
    "        token_freq_set = token_freq.most_common(vocab_size-1)\n",
    "        vocab =[k for k, v in token_freq_set if v>=min_freq]\n",
    "        if 'UNK' not in vocab:\n",
    "            vocab.append('UNK')\n",
    "        return vocab\n",
    "    \n",
    "    def token2ix(self, text, vocab_size=10000, min_freq=2):\n",
    "        vocab = self.__buildVocabulary(text, vocab_size, min_freq)\n",
    "        #Map each char into an ID\n",
    "        word_id_map = dict(zip(vocab, range(len(vocab))))\n",
    "        #Map each ID into a word\n",
    "        id_word_map = dict(zip(word_id_map.values(), word_id_map.keys()))\n",
    "        return word_id_map, id_word_map, vocab\n",
    "        \n",
    "    def sent2ids(self, word_id_map, sent):\n",
    "        def word2id(c):\n",
    "            try:\n",
    "               ID = word_id_map[c]\n",
    "            except:#Trun those less frequent words into UNK\n",
    "               ID = word_id_map['UNK']\n",
    "            return ID\n",
    "        \n",
    "        ids = [word2id(item) for item in sent]\n",
    "        return ids\n",
    "    \n",
    "    def sents2sent(self, word_id_map, sents):\n",
    "        sents_ids = []\n",
    "        for sent in sents:       \n",
    "            ids = self.sent2ids(word_id_map, sent)\n",
    "            sents_ids.append(ids)\n",
    "        return sents_ids\n",
    "        \n",
    "    def texts2sents(self, word_id_map, texts):\n",
    "        texts_ids = []\n",
    "        for text in texts:       \n",
    "            ids = self.sents2sent(word_id_map, text)\n",
    "            texts_ids.append(ids)\n",
    "        return texts_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class dataTransform(text2ids):\n",
    "    '''\n",
    "    An object of news data\n",
    "    Transform texts into lists of ids\n",
    "    '''\n",
    "    def __init__(self, train_text, test_text, vocab_size=10000, min_freq=2):\n",
    "        self.train_text = train_text\n",
    "        self.test_text = test_text\n",
    "        self.vocab_size= vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.__buildVocabulary()\n",
    "    \n",
    "    def __buildVocabulary(self):\n",
    "        '''Unlist all the nested list and build a vocabulary'''\n",
    "        self.word_id_map, self.id_word_map, self.vocab = self.token2ix(self.train_text, \n",
    "                                                                                self.vocab_size, \n",
    "                                                                                self.min_freq)\n",
    "        \n",
    "    def text2ids(self):\n",
    "        '''\n",
    "        Transform texts into sentences of words\n",
    "        '''\n",
    "        train_sents_ids = self.texts2sents(self.word_id_map, self.train_text)\n",
    "        test_sents_ids = self.texts2sents(self.word_id_map, self.test_text)\n",
    "        return train_sents_ids, test_sents_ids\n",
    "    \n",
    "    def vocabularyMap(self):\n",
    "        return self.word_id_map, self.id_word_map, self.vocab\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hier_train_text, hier_train_label = list(zip(*hier_train_data))\n",
    "hier_test_text, hier_test_label = list(zip(*hier_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform texts of hierarchical lists of words\n",
    "dt = dataTransform(hier_train_text, hier_test_text)\n",
    "train_sents_ids, test_sents_ids = dt.text2ids()\n",
    "word_id_map, id_word_map, vocab = dt.vocabularyMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have transform words into IDs, we can take a look at the lengths of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents_lengths = []\n",
    "for text in train_sents_ids:\n",
    "    for sents in text:\n",
    "            sents_lengths.append(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length 1076\n",
      "Min Length 1\n"
     ]
    }
   ],
   "source": [
    "print('Max Length', max(sents_lengths))\n",
    "print('Min Length', min(sents_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.  ,   6.  ,   9.  ,  16.  ,  25.  ,  70.88])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(sents_lengths, [5, 15, 25, 50, 75, 99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sentences have less than 100 words whereas the maximum length is 8263 and the minimum is 1, quite unusual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sent_len = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def judgeLength(data):\n",
    "    text = data[0]\n",
    "    lengths = []\n",
    "    #print(text)\n",
    "    for sents in text:\n",
    "        lengths.append(len(sents))\n",
    "    flag = max(lengths) < max_sent_len\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress the text IDs and labels into one list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hier_train_vecs = list(zip(train_sents_ids, hier_train_label))\n",
    "hier_test_vecs = list(zip(test_sents_ids, hier_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Filter those news with extreme long sentences\n",
    "hier_train_vecs = list(filter(judgeLength, hier_train_vecs))\n",
    "hier_test_vecs = list(filter(judgeLength, hier_test_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(len, train_sents_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A news can have as many as 49 sentences atmost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of Training Data: 9903  Counts of Testing Data: 6648\n"
     ]
    }
   ],
   "source": [
    "print('Counts of Training Data:', len(hier_train_vecs), ' Counts of Testing Data:', len(hier_test_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the performances of our models, we need to divide our training set into training part and validation part. The validation part of texts share the same distributions of training part, so it is likely to use them to check whether the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "news_vecs, news_labels = list(zip(*hier_train_vecs))\n",
    "test_news_vecs, test_news_labels = list(zip(*hier_test_vecs))\n",
    "training_text_vecs, validation_text_vecs, training_text_labels, validation_text_labels = \\\n",
    "                                                              train_test_split(news_vecs, news_labels, test_size=160, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hier_train_input = list(zip(training_text_vecs, training_text_labels))\n",
    "hier_valid_input = list(zip(validation_text_vecs, validation_text_labels))\n",
    "hier_test_input = list(zip(test_news_vecs, test_news_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6648"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hier_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save files in local disk\n",
    "import pandas as pd\n",
    "train_data_processed = {'text':training_text_vecs, 'label':training_text_labels}\n",
    "valid_data_processed = {'text':validation_text_vecs, 'label':validation_text_labels}\n",
    "test_data_processed = {'text':test_news_vecs, 'label':test_news_labels}\n",
    "td = pd.DataFrame(train_data_processed)\n",
    "td.to_csv('training_data_processed.csv', index=False)\n",
    "td = pd.DataFrame(test_data_processed)\n",
    "td.to_csv('testing_data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2id(c):\n",
    "    try:\n",
    "        ID = word_id_map[c]\n",
    "    except:#Trun those less frequent words into UNK\n",
    "        ID = word_id_map['UNK']\n",
    "    return ID\n",
    "def id2word(c):\n",
    "    try:\n",
    "        word = id_word_map[c]\n",
    "    except:\n",
    "        word='UNK'\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the length varies much, perhaps we need buckets to put news with similar lengths together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batch Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class generateSamples:\n",
    "    '''Generate samples for training and testing'''\n",
    "    \n",
    "    def __init__(self, hier_data, max_len=100):\n",
    "        '''Pass batch size and poems vectors'''\n",
    "        news_vecs, news_labels = list(zip(*hier_data))\n",
    "        self.index = 0\n",
    "        self.news_vecs = news_vecs\n",
    "        self.news_labels = news_labels\n",
    "        self.news_count = len(news_vecs)\n",
    "        self.max_sent_len = max_len\n",
    "        \n",
    "    def generate_batch(self, batch_size=64, is_training=True):\n",
    "        '''Generate a training sample each time'''\n",
    "        \n",
    "        selected_samples = []\n",
    "        selected_labels = []\n",
    "        batch_weights = []\n",
    "        #For training, select random samples\n",
    "        if is_training:\n",
    "            selected_index = np.random.choice(len(self.news_vecs), batch_size, replace=True)\n",
    "            for index in selected_index:\n",
    "                selected_samples.append(self.news_vecs[index])\n",
    "                selected_labels.append(self.news_labels[index])\n",
    "\n",
    "        #For testing, select a few samples each time\n",
    "        else:#Testing model\n",
    "            start = self.index%self.news_count\n",
    "            end = (start + batch_size)%self.news_count\n",
    "            #In case end goes beyong the range of the samples\n",
    "            if end > start:\n",
    "                selected_samples = self.news_vecs[start: end]\n",
    "                selected_labels = self.news_labels[start: end]\n",
    "                self.index = end\n",
    "            else:\n",
    "                print('Test Samples come to an end!')\n",
    "                selected_samples = self.news_vecs[start: ]\n",
    "                selected_labels = self.news_labels[start: ]\n",
    "                self.index = 0\n",
    "            \n",
    "        #Find the maximun number of sentences in one text\n",
    "        max_sent_count = max(map(len, selected_samples))\n",
    "        #Find the longest sentence and record the length\n",
    "        find_max_len = lambda x: max(map(len, x))\n",
    "        max_sent_length = max(map(find_max_len, selected_samples))\n",
    "        #Record sentence number for each text\n",
    "        max_sent_count = 49\n",
    "        #Get the max length in current batch\n",
    "        #Three level lengths\n",
    "        text_sent_lengths = []\n",
    "        text_sent_vectors = []\n",
    "        for sample in selected_samples:\n",
    "            #Create input and label\n",
    "            sent_lengths = []\n",
    "            sent_vectors = []\n",
    "            for sent in sample:\n",
    "                x = np.full((self.max_sent_len), word2id('UNK'), np.int32)\n",
    "                #Record lengths of each sentence\n",
    "                sent_lengths.append(len(sent))\n",
    "                #Transform a list to a vector\n",
    "                x[:len(sent)] = np.array(sent)\n",
    "                sent_vectors.append(x)\n",
    "            #Record each sentence information for each text\n",
    "            text_sent_lengths.append(sent_lengths)\n",
    "            text_sent_vectors.append(sent_vectors)\n",
    "            \n",
    "        text_sent_labels = np.array(selected_labels)\n",
    "\n",
    "        return text_sent_vectors, text_sent_labels, text_sent_lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generateTextSequences:\n",
    "    '''Generate hierarchical samples for training and testing'''\n",
    "    \n",
    "    def __init__(self, hier_data, max_sent_len=75, max_sent_count=49):\n",
    "        '''Pass batch size and poems vectors'''\n",
    "        news_vecs, news_labels = list(zip(*hier_data))\n",
    "        self.index = 0\n",
    "        self.news_vecs = news_vecs\n",
    "        self.news_labels = news_labels\n",
    "        self.news_count = len(news_vecs)\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.max_sent_count = max_sent_count\n",
    "        \n",
    "    def generate_batch(self, batch_size=32, is_training=True):\n",
    "        '''Generate a training sample each time'''\n",
    "        \n",
    "        selected_samples = []\n",
    "        selected_labels = []\n",
    "        batch_weights = []\n",
    "        #For training, select random samples\n",
    "        if is_training:\n",
    "            selected_index = np.random.choice(len(self.news_vecs), batch_size, replace=True)\n",
    "            for index in selected_index:\n",
    "                selected_samples.append(self.news_vecs[index])\n",
    "                selected_labels.append(self.news_labels[index])\n",
    "\n",
    "        #For testing, select a few samples each time\n",
    "        else:#Testing model\n",
    "            start = self.index%self.news_count\n",
    "            end = (start + batch_size)%self.news_count\n",
    "            #In case end goes beyong the range of the samples\n",
    "            if end > start:\n",
    "                selected_samples = self.news_vecs[start: end]\n",
    "                selected_labels = self.news_labels[start: end]\n",
    "                self.index = end\n",
    "            else:\n",
    "                print('Samples come to an end!')\n",
    "                selected_samples = self.news_vecs[start: ]\n",
    "                selected_labels = self.news_labels[start: ]\n",
    "                self.index = 0\n",
    "            \n",
    "        #Find the maximun number of sentences in one text\n",
    "        #max_sent_count = max(map(len, selected_samples))\n",
    "        max_sent_count = self.max_sent_count\n",
    "        #Find the longest sentence and record the length\n",
    "        find_max_len = lambda x: max(map(len, x))\n",
    "        #max_sent_length = max(map(find_max_len, selected_samples))\n",
    "        max_sent_length = self.max_sent_len\n",
    "        #Get the max length in current batch\n",
    "        #Three level lengths\n",
    "        text_sent_vectors = np.full((batch_size, max_sent_count, max_sent_length), word2id('UNK'), np.int32)\n",
    "        text_sent_lengths = np.full((batch_size, max_sent_count), 0, np.int32)\n",
    "        for i, sample in enumerate(selected_samples):\n",
    "            #Create input and label\n",
    "            for j, sent in enumerate(sample):\n",
    "                #Record lengths of each sentence\n",
    "                text_sent_lengths[i, j] = len(sent)\n",
    "                #Transform a list to a vector\n",
    "                text_sent_vectors[i, j, :len(sent)] = np.array(sent)\n",
    "            \n",
    "        text_sent_labels = np.array(selected_labels)\n",
    "\n",
    "        return text_sent_vectors, text_sent_labels, text_sent_lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_train = generateTextSequences(hier_train_input)\n",
    "train_text_vectors, train_text_labels, train_text_lengths = gs_train.generate_batch()\n",
    "gs_test = generateTextSequences(hier_test_input)\n",
    "test_text_vectors, test_text_labels, test_text_lengths = gs_test.generate_batch(is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data exploration, it is clear that the length of the news varies much, ranging from 10 to 10000. In order to deal with that case, we can take buckets into consideration, similar to seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class trainConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_sent_len = 82\n",
    "    label_size = 20\n",
    "    embed_size = 64\n",
    "    batch_size = 16\n",
    "    layer_size = 2\n",
    "    max_sent_count = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class testConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_sent_len = 82\n",
    "    label_size = 20\n",
    "    embed_size = 64\n",
    "    batch_size = 16\n",
    "    layer_size = 2\n",
    "    max_sent_count = 49\n",
    "    \n",
    "class singleConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_sent_len = 82\n",
    "    label_size = 20\n",
    "    embed_size = 64\n",
    "    batch_size = 1\n",
    "    layer_size = 2\n",
    "    max_sent_count = 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Hierarchical RNN Model\n",
    "\n",
    "In this model, we view each text as a series of sentences and each sentence consists of a sereies of words. First, we use word vectors of a sentence to feed LSTM to generate a sentence vector, then we use sentence vectors to generate a text vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from tensorflow.contrib.layers.python.layers import encoders\n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reference:http://blog.csdn.net/u010223750/article/details/71079036\n",
    "from tensorflow.contrib import rnn\n",
    "class HierarchicalRNN_Model:\n",
    "    def __init__(self, config, x, y, lengths, is_training=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_size = config.embed_size\n",
    "        self.label_size = config.label_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.lengths = lengths\n",
    "        self.max_sent_len = config.max_sent_len\n",
    "        self.max_sent_count = config.max_sent_count\n",
    "        self.is_training = is_training\n",
    "        self.predict\n",
    "        if is_training:\n",
    "            self.optimize\n",
    "        print('Model Initialized!')\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        logits = self.inference\n",
    "        targets = tf.one_hot(self.y, 20, 1, 0)\n",
    "        targets = tf.cast(targets, tf.float32)\n",
    "        #Note  tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=activation)\n",
    "        loss = tf.losses.softmax_cross_entropy(targets, logits)\n",
    "        return loss\n",
    "    \n",
    "    @lazy_property\n",
    "    def predict(self):\n",
    "        logits = self.inference\n",
    "        #probs = tf.nn.softmax(logits)\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        return predictions\n",
    "    \n",
    "    @lazy_property\n",
    "    def correct_num(self):\n",
    "        prediction = self.predict\n",
    "        targets = tf.reshape(self.y, [-1])\n",
    "        targets = tf.cast(targets, tf.int64)\n",
    "        correct_prediction = tf.equal(prediction, targets)\n",
    "        correct_num = tf.reduce_sum(tf.cast(correct_prediction, \"float\"))\n",
    "        return correct_num\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            cost = self.cost\n",
    "        #with tf.name_scope('Optimizer'):\n",
    "            #self._learning_rate = tf.Variable(0.0, trainable=False)\n",
    "            train_op = tf.train.AdamOptimizer(0.0005).minimize(cost)\n",
    "            #train_op = tf.train.AdamOptimizer(self._learning_rate).minimize(cost)\n",
    "            #tvars = tf.trainable_variables()\n",
    "            #grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 6)\n",
    "            #optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "            #train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return train_op\n",
    "    \n",
    "            \n",
    "    \n",
    "    @lazy_property\n",
    "    def inference(self):\n",
    "        '''\n",
    "        Transform each text into a vector on basis of sentence vectors\n",
    "        '''\n",
    "        #Create embedding matrix\n",
    "        with tf.variable_scope('Embeddings'):\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embeddings = tf.get_variable('embedding', [self.vocab_size,  self.embed_size])\n",
    "                inputs = tf.nn.embedding_lookup(embeddings, self.x)\n",
    "        if self.is_training:\n",
    "            inputs = tf.nn.dropout(inputs, 0.5)\n",
    "\n",
    "        #slice inputs into a series of vectors\n",
    "        #batch_doc_vectors = tf.unstack(inputs, axis=0)\n",
    "        def lstm():\n",
    "            cell = rnn.BasicLSTMCell(self.embed_size, forget_bias=0.0, \n",
    "                                      state_is_tuple=True) \n",
    "            if self.is_training:\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "            return cell\n",
    "        with tf.variable_scope('RNN_Word_Layer'):      \n",
    "            #cell = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], \n",
    "                                #state_is_tuple=True)\n",
    "            cell_word = rnn.MultiRNNCell([lstm()],  state_is_tuple=True)\n",
    "\n",
    "        #Get the size\n",
    "        #(document_size, sentence_size, word_size) = tf.unstack(tf.shape(self.x))\n",
    "        #Dynamic RNN with given lengths for each text\n",
    "        #Get RNN vector for each sentence\n",
    "        text_states = []\n",
    "        for i in range(self.batch_size):\n",
    "            input_text = inputs[i]\n",
    "            initial_state_word = cell_word.zero_state((self.max_sent_count), tf.float32)\n",
    "            #Pass the lengths of each sentence\n",
    "            outputs_word, status_word = tf.nn.dynamic_rnn(cell_word, input_text, initial_state=initial_state_word,\n",
    "                                                sequence_length=self.lengths[i], dtype=tf.float32)\n",
    "            \n",
    "            #Get the final hidden state\n",
    "            final_status_word = status_word[-1].h\n",
    "            #For debugging\n",
    "            #self.final_status = final_status\n",
    "            #print(status_concat)\n",
    "            #Use mean hidden state of sent states as the text hidden state\n",
    "            #text_hidden_state = tf.reduce_mean(final_status, axis=0)\n",
    "            #Use sent hidden states to feed another LSTM\n",
    "            text_states.append(final_status_word)\n",
    "        #Pass the sentence hidden vector into a LSTM\n",
    "        text_states = tf.stack(text_states)\n",
    "        #Get the number of sentences of each text\n",
    "        great_num = tf.greater(self.lengths, 0)\n",
    "        great_num = tf.cast(great_num, tf.int32)\n",
    "        text_sent_num = tf.reduce_sum(great_num, axis=1)\n",
    "        #Now view a text as a series of sentence vectors\n",
    "        with tf.variable_scope('RNN_Sent_Layer'):      \n",
    "            #cell = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], \n",
    "                                #state_is_tuple=True)\n",
    "            cell_sent = rnn.MultiRNNCell([lstm()],  state_is_tuple=True)\n",
    "            initial_state_sent = cell_sent.zero_state((self.batch_size), tf.float32)\n",
    "            outputs_sent, status_sent = tf.nn.dynamic_rnn(cell_sent, text_states, initial_state=initial_state_sent,\n",
    "                                                      sequence_length=text_sent_num,\n",
    "                                                      dtype=tf.float32)\n",
    "                \n",
    "            \n",
    "        output = status_sent[-1].h\n",
    "        \n",
    "        #output = outputs[:,-1,:]\n",
    "        #If we use padding, the last output will be based on the padding input values\n",
    "        #Note here we use the hidden state instead of the last output\n",
    "        #In dynamic rnn, the last state will remain the same after specified time steps\n",
    "        #For example, if the length is 10 and the padding sequence has 20 words, the \n",
    "        #final state will be the one of 10th time step\n",
    "        #Multilayer RNN has multilayer hidden states\n",
    "        #output = status[1].h\n",
    "        \n",
    "        \n",
    "        weights = tf.get_variable('weights', [self.embed_size, self.label_size], dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases', [self.label_size], dtype=tf.float32)\n",
    "        logits = tf.matmul(output, weights) + biases\n",
    "        #\n",
    "        return logits\n",
    "    \n",
    "    @property\n",
    "    def learningRate(self):\n",
    "        return self._learning_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "graph_rnn = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_rnn.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.02, 0.02)\n",
    "    with tf.name_scope('train'):\n",
    "        train_texts = tf.placeholder(tf.int32, [trainConfig.batch_size, None, None])\n",
    "        train_label = tf.placeholder(tf.int32, [trainConfig.batch_size])\n",
    "        train_lengths = tf.placeholder(tf.float32, [trainConfig.batch_size, None])\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            train_model = HierarchicalRNN_Model(trainConfig, train_texts, train_label, train_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "with graph_rnn.as_default():\n",
    "    with tf.name_scope('test'):\n",
    "        test_texts = tf.placeholder(tf.int32, [testConfig.batch_size, None, None])\n",
    "        test_label = tf.placeholder(tf.int32, [testConfig.batch_size])\n",
    "        test_lengths = tf.placeholder(tf.float32, [testConfig.batch_size, None])\n",
    "        single_text = tf.placeholder(tf.int32, [1, None, None])\n",
    "        single_label = tf.placeholder(tf.int32, [1])\n",
    "        single_lengths = tf.placeholder(tf.float32, [1, None])\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = HierarchicalRNN_Model(testConfig, test_texts, test_label, test_lengths, False)\n",
    "            single_model = HierarchicalRNN_Model(singleConfig, single_text, single_label, single_lengths, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num = int(len(hier_train_input)/trainConfig.batch_size)\n",
    "test_chunk_num = int(len(hier_test_input)/testConfig.batch_size)\n",
    "valid_chunk_num = int(len(hier_valid_input)/testConfig.batch_size)\n",
    "remain_num = len(hier_test_input) - testConfig.batch_size*test_chunk_num\n",
    "remain_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_train = generateTextSequences(hier_train_input)\n",
    "train_text_vectors, train_text_labels, train_text_lengths = gs_train.generate_batch(batch_size=trainConfig.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_validation(gs, chunk_num):\n",
    "    #gs_valid = generateTextSequences(hier_valid_data)\n",
    "    count = 0\n",
    "    for _ in range(chunk_num):\n",
    "        valid_text_vectors, valid_text_labels, valid_text_lengths = \\\n",
    "                                                       gs.generate_batch(batch_size=testConfig.batch_size, \n",
    "                                                                               is_training=False)\n",
    "        feed_dict = {test_texts:valid_text_vectors, test_label:valid_text_labels, \n",
    "                         test_lengths:valid_text_lengths}\n",
    "        num = sess.run([test_model.correct_num], feed_dict=feed_dict)\n",
    "        count += np.sum(num)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.9975\n",
      "Loss: 2.6574\n",
      "Loss: 2.5238\n",
      "Validating\n",
      "Samples come to an end!\n",
      "Validation Accuracy:0.1313\n",
      "Epoch 1 time:672.91\n",
      "Loss: 2.2751\n",
      "Loss: 2.3032\n",
      "Loss: 2.4473\n",
      "Validating\n",
      "Samples come to an end!\n",
      "Validation Accuracy:0.1000\n",
      "Epoch 2 time:674.12\n",
      "Testing...\n",
      "Testing Time:90.16\n",
      "Testing Accuracy: 0.109487951807\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = 2\n",
    "with tf.Session(graph=graph_rnn) as sess:\n",
    "    #Initialize parameters\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for m in range(epochs):\n",
    "        #Training\n",
    "        for i in range(train_chunk_num):\n",
    "            #sess.run(tf.assign(learning_rate, 0.002*((0.98)**m)))\n",
    "            train_text_vectors, train_text_labels, train_text_lengths = \\\n",
    "                                                       gs_train.generate_batch(batch_size=trainConfig.batch_size)\n",
    "            feed_dict = {train_texts:train_text_vectors, train_label:train_text_labels, \n",
    "                         train_lengths:train_text_lengths}\n",
    "            l, _ = sess.run([train_model.cost, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%300 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "        #Validating\n",
    "        print('Validating')\n",
    "        gs_valid = generateTextSequences(hier_valid_input)\n",
    "        count = model_validation(gs_valid, valid_chunk_num)\n",
    "        print('Validation Accuracy:{:.4f}'.format(count/len(hier_valid_input)))\n",
    "        end_time = time.time()\n",
    "        print('Epoch', m+1, 'time:{:.2f}'.format(end_time - start_time))\n",
    "        start_time = end_time\n",
    "        \n",
    "    #Calculate Testing Accuracy\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    gs_test = generateTextSequences(hier_test_input)\n",
    "    count = model_validation(gs_test, test_chunk_num)\n",
    "    \n",
    "    #for _ in range(test_chunk_num):\n",
    "        #Traverse each data\n",
    "        #gs_test = generateTextSequences(hier_test_input)\n",
    "        #test_text_vectors, test_text_labels, test_text_lengths = gs_test.generate_batch(batch_size=testConfig.batch_size, \n",
    "                                                                                #is_training=False)\n",
    "        #feed_dict = {test_texts:test_text_vectors, test_label:test_text_labels, \n",
    "                         #test_lengths:test_text_lengths}\n",
    "        #n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        #count += np.sum(n)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print('Testing Accuracy:', count*1.0/test_chunk_num/testConfig.batch_size)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems Hierarchical RNN networks perform better than simple RNNnetworks, pehaps it considers the structure of the texts."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
